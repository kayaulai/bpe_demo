{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Lesson 1: Subword tokenization using byte pair encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kayau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea of subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initial steps\n",
    "\n",
    "Let's say that these are the inputs we'll be giving the BPE algorithm, including a corpus and a desired vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The sink stinks.\", \"Sit in the egg.\",\n",
    "\"The tinker hikes.\", \"The hiker kisses the stinking sink.\"]\n",
    "V = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1(a): We kick off the process by preprocessing the corpus, splitting it by words and removing case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the#sink#stinks#.',\n",
       " 'sit#in#the#egg#.',\n",
       " 'the#tinker#hikes#.',\n",
       " 'the#hiker#kisses#the#stinking#sink#.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    \"\"\"Preprocess a corpus by turning everything lowercase and\n",
    "    splitting it into words.\n",
    "\n",
    "    Args:\n",
    "        corpus (list(chr)): A list of texts.\n",
    "\n",
    "    Returns:\n",
    "        list(chr): A list of texts with the pound sign (#) between words.\n",
    "    \"\"\"\n",
    "    processed_corpus = [\"\"] * len(corpus)\n",
    "    for (i, text) in enumerate(corpus):\n",
    "        processed_corpus[i] = preprocess_text(text)\n",
    "    return processed_corpus\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess a corpus by turning everything lowercase and\n",
    "    splitting it into words.\n",
    "\n",
    "    Args:\n",
    "        corpus (chr): A text.\n",
    "\n",
    "    Returns:\n",
    "        list(chr): A list of words from the input text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    return \"#\".join(word_tokenize(text))\n",
    "\n",
    "corpus_preprocessed = preprocess_corpus(corpus)\n",
    "corpus_preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1(b): We get the initial tokenisation by splitting the text into tokens of single characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e # s i n k # s t i n k s # .\n",
      "s i t # i n # t h e # e g g # .\n",
      "t h e # t i n k e r # h i k e s # .\n",
      "t h e # h i k e r # k i s s e s # t h e # s t i n k i n g # s i n k # .\n",
      "t h e # s i n k # s t i n k s # .\n",
      "s i t # i n # t h e # e g g # .\n",
      "t h e # t i n k e r # h i k e s # .\n",
      "t h e # h i k e r # k i s s e s # t h e # s t i n k i n g # s i n k # .\n"
     ]
    }
   ],
   "source": [
    "def get_init_tokenization(corpus):\n",
    "    \"\"\"Get an initial tokenized corpus where each\n",
    "       character is a token.\n",
    "\n",
    "    Args:\n",
    "        corpus (list(str)): A list of preprocessed texts.                            \n",
    "\n",
    "    Returns:\n",
    "        list(str): A list of lists, each of which is a single-\n",
    "                   character token.\n",
    "    \"\"\"\n",
    "    corpus_tokenized = [None] * len(corpus)\n",
    "    for (i, text) in enumerate(corpus):\n",
    "        corpus_tokenized[i] = list(text)\n",
    "    return corpus_tokenized\n",
    "\n",
    "def print_tokenized_corpus(corpus, include_pound = True):\n",
    "    \"\"\"Prints a tokenized corpus with a space between each token\n",
    "    and a line break between two texts.\n",
    "\n",
    "    Args:\n",
    "        corpus (list(list(str))): A list of texts in the form of\n",
    "                                  lists of tokens.        \n",
    "    \"\"\"\n",
    "    joined_texts = [\"\"] * len(corpus)\n",
    "    for (i, text) in enumerate(corpus):\n",
    "        joined_text = \" \".join(text)\n",
    "        if not include_pound:\n",
    "            joined_text.replace(\" #\", \"\")\n",
    "        joined_texts[i] = joined_text\n",
    "    print(\"\\n\".join(joined_texts))\n",
    "    \n",
    "corpus_tokenized = get_init_tokenization(corpus_preprocessed)\n",
    "print_tokenized_corpus(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1(c): Get the vocabulary from the initial tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', 'e', 'g', 'h', 'i', 'k', 'n', 'r', 's', 't'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(tokenized_corpus):\n",
    "    \"\"\"Get the vocabulary from a tokenzied corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus (list(list(str))): A list of texts in the form of\n",
    "                                  lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "        set(str): A set of types in the vocabulary.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for text in tokenized_corpus:\n",
    "        vocab = vocab.union(set(text))\n",
    "    vocab.remove(\"#\")\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(corpus_tokenized)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Main processing step\n",
    "\n",
    "In each iteration of the main processing step, we need to find all the bigrams in the corpus, count them, get the most frequent one, and merge it in the corpus.\n",
    "\n",
    "Let's first count all the bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('i', 'n'): 7, ('t', 'h'): 5, ('h', 'e'): 5, ('n', 'k'): 5, ('s', 'i'): 3, ('t', 'i'): 3, ('k', 'e'): 3, ('s', 't'): 2, ('e', 'r'): 2, ('h', 'i'): 2, ('i', 'k'): 2, ('e', 's'): 2, ('k', 'i'): 2, ('k', 's'): 1, ('i', 't'): 1, ('e', 'g'): 1, ('g', 'g'): 1, ('i', 's'): 1, ('s', 's'): 1, ('s', 'e'): 1, ('n', 'g'): 1})\n",
      "('i', 'n')\n",
      "Counter({('i', 'n'): 7, ('t', 'h'): 5, ('h', 'e'): 5, ('n', 'k'): 5, ('s', 'i'): 3, ('t', 'i'): 3, ('k', 'e'): 3, ('s', 't'): 2, ('e', 'r'): 2, ('h', 'i'): 2, ('i', 'k'): 2, ('e', 's'): 2, ('k', 'i'): 2, ('k', 's'): 1, ('i', 't'): 1, ('e', 'g'): 1, ('g', 'g'): 1, ('i', 's'): 1, ('s', 's'): 1, ('s', 'e'): 1, ('n', 'g'): 1})\n",
      "('i', 'n')\n"
     ]
    }
   ],
   "source": [
    "def count_bigrams_in_corpus(tokenized_corpus):\n",
    "    \"\"\"Finds all bigrams in a corpus of texts.\n",
    "\n",
    "    Args:\n",
    "        tokenized_corpus (list(list(str))): A list of texts in the form of\n",
    "                                  lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the bigrams found in one text in the corpus.\"\"\"\n",
    "\n",
    "    bigrams_all = list()\n",
    "    for text in tokenized_corpus:\n",
    "        bigrams_all = bigrams_all + (find_bigrams_in_text(text))\n",
    "    return collections.Counter(bigrams_all)\n",
    "\n",
    "def find_bigrams_in_text(tokenized_text):\n",
    "    \"\"\"Find all bigrams in a tokenized text.\n",
    "\n",
    "    Args:\n",
    "        tokenized_text (list(str)): A list of tokens representing\n",
    "                                    a text.\n",
    "\n",
    "    Returns:\n",
    "        list(tuple(str)): A list of tuples representing bigrams. \n",
    "    \"\"\"\n",
    "    bigrams_list = list()\n",
    "    for i in range(len(tokenized_text) - 1):\n",
    "        if tokenized_text[i] == \"#\" or tokenized_text[i + 1] == \"#\":\n",
    "            continue\n",
    "        bigrams_list.append((tokenized_text[i], tokenized_text[i + 1]))\n",
    "    return bigrams_list\n",
    "\n",
    "bigrams_merged = list()\n",
    "bigrams = count_bigrams_in_corpus(corpus_tokenized)\n",
    "print(bigrams)\n",
    "top_bigram = bigrams.most_common()[0][0]\n",
    "bigrams_merged.append(top_bigram)\n",
    "print(top_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to merge the bigrams. Let's first write a function to merge two items within a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 't', 'in', 'k']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_strlist_items(strlist, i):\n",
    "    \"\"\"Merge two tokens inside a list.\n",
    "\n",
    "    Args:\n",
    "        strlist (_type_): A list of tokens.\n",
    "        i (int): The index of the first item to be merged.\n",
    "\n",
    "    Returns:\n",
    "        list(str): The new list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    j = i + 1\n",
    "    strlist[i] = strlist[i] + strlist[j]\n",
    "    strlist.pop(j)\n",
    "    return strlist\n",
    "\n",
    "merge_strlist_items([\"s\", \"t\", \"i\", \"n\", \"k\"], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the desired bigram in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e # s in k # s t in k s # .\n",
      "s i t # in # t h e # e g g # .\n",
      "t h e # t in k e r # h i k e s # .\n",
      "t h e # h i k e r # k i s s e s # t h e # s t in k in g # s in k # .\n",
      "t h e # s in k # s t in k s # .\n",
      "s i t # in # t h e # e g g # .\n",
      "t h e # t in k e r # h i k e s # .\n",
      "t h e # h i k e r # k i s s e s # t h e # s t in k in g # s in k # .\n"
     ]
    }
   ],
   "source": [
    "def find_bigram_in_text(tokenized_text, bigram):\n",
    "    \"\"\"Find the location of a bigram in a text.\n",
    "        If none are found, return None.\"\"\"\n",
    "    for i in range(len(tokenized_text) - 1):\n",
    "        if tokenized_text[i] == bigram[0] and\\\n",
    "            tokenized_text[i + 1] == bigram[1]:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def merge_bigram_in_text(tokenized_text, bigram):\n",
    "    \"\"\"Merge a specified bigram in a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokenized_text (list(str)): The list of tokens to be retokenized.\n",
    "        bigram (tuple): The bigram to be merged.\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    while not done:\n",
    "        bigram_loc = find_bigram_in_text(tokenized_text, bigram)\n",
    "        if bigram_loc is None:\n",
    "            done = True\n",
    "            break\n",
    "        merge_strlist_items(tokenized_text, bigram_loc)\n",
    "\n",
    "def merge_bigram_in_corpus(tokenized_corpus, bigram):\n",
    "    \"\"\"Merge a specified bigram in a corpus.\n",
    "\n",
    "    Args:\n",
    "        tokenized_corpus (list(list(str))): The list of tokens to be retokenized.\n",
    "        bigram (tuple): The bigram to be merged.\n",
    "    \"\"\"\n",
    "    for text in tokenized_corpus:\n",
    "        merge_bigram_in_text(text, bigram)\n",
    "\n",
    "merge_bigram_in_corpus(corpus_tokenized, top_bigram)\n",
    "print_tokenized_corpus(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final substep is to update the vocabulary to contain the new subword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h', '.', 'g', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "{'h', '.', 'g', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n"
     ]
    }
   ],
   "source": [
    "vocab.add(top_bigram[0] + top_bigram[1])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all those together into a function, and tuck it into a `while` loop so that it keeps running until the vocabulary size has reached *V*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1====\n",
      "Top bigram: (('t', 'h'), 5)\n",
      "Corpus after merge:\n",
      "th e # s in k # s t in k s # .\n",
      "s i t # in # th e # e g g # .\n",
      "th e # t in k e r # h i k e s # .\n",
      "th e # h i k e r # k i s s e s # th e # s t in k in g # s in k # .\n",
      "Vocab after merge (size 12): {'h', '.', 'th', 'g', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "====Iteration 2====\n",
      "Top bigram: (('th', 'e'), 5)\n",
      "Corpus after merge:\n",
      "the # s in k # s t in k s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # t in k e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s t in k in g # s in k # .\n",
      "Vocab after merge (size 13): {'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "====Iteration 3====\n",
      "Top bigram: (('in', 'k'), 5)\n",
      "Corpus after merge:\n",
      "the # s ink # s t ink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # t ink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s t ink in g # s ink # .\n",
      "Vocab after merge (size 14): {'ink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "====Iteration 4====\n",
      "Top bigram: (('t', 'ink'), 3)\n",
      "Corpus after merge:\n",
      "the # s ink # s tink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s tink in g # s ink # .\n",
      "Vocab after merge (size 15): {'ink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 5====\n",
      "Top bigram: (('s', 'ink'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # s tink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s tink in g # sink # .\n",
      "Vocab after merge (size 16): {'sink', 'ink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 6====\n",
      "Top bigram: (('s', 'tink'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 17): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 7====\n",
      "Top bigram: (('e', 'r'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # h i k e s # .\n",
      "the # h i k er # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 18): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 8====\n",
      "Top bigram: (('h', 'i'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # hi k e s # .\n",
      "the # hi k er # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 19): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 'hi', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 9====\n",
      "Top bigram: (('hi', 'k'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # hik e s # .\n",
      "the # hik er # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 20): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 'hi', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k', 'hik'}\n",
      "====Iteration 1====\n",
      "Top bigram: (('t', 'h'), 5)\n",
      "Corpus after merge:\n",
      "th e # s in k # s t in k s # .\n",
      "s i t # in # th e # e g g # .\n",
      "th e # t in k e r # h i k e s # .\n",
      "th e # h i k e r # k i s s e s # th e # s t in k in g # s in k # .\n",
      "Vocab after merge (size 12): {'h', '.', 'th', 'g', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "====Iteration 2====\n",
      "Top bigram: (('th', 'e'), 5)\n",
      "Corpus after merge:\n",
      "the # s in k # s t in k s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # t in k e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s t in k in g # s in k # .\n",
      "Vocab after merge (size 13): {'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "====Iteration 3====\n",
      "Top bigram: (('in', 'k'), 5)\n",
      "Corpus after merge:\n",
      "the # s ink # s t ink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # t ink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s t ink in g # s ink # .\n",
      "Vocab after merge (size 14): {'ink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'k'}\n",
      "====Iteration 4====\n",
      "Top bigram: (('t', 'ink'), 3)\n",
      "Corpus after merge:\n",
      "the # s ink # s tink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s tink in g # s ink # .\n",
      "Vocab after merge (size 15): {'ink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 5====\n",
      "Top bigram: (('s', 'ink'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # s tink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # s tink in g # sink # .\n",
      "Vocab after merge (size 16): {'sink', 'ink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 6====\n",
      "Top bigram: (('s', 'tink'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink e r # h i k e s # .\n",
      "the # h i k e r # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 17): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 7====\n",
      "Top bigram: (('e', 'r'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # h i k e s # .\n",
      "the # h i k er # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 18): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 8====\n",
      "Top bigram: (('h', 'i'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # hi k e s # .\n",
      "the # hi k er # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 19): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 'hi', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k'}\n",
      "====Iteration 9====\n",
      "Top bigram: (('hi', 'k'), 2)\n",
      "Corpus after merge:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # hik e s # .\n",
      "the # hik er # k i s s e s # the # stink in g # sink # .\n",
      "Vocab after merge (size 20): {'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 'hi', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k', 'hik'}\n"
     ]
    }
   ],
   "source": [
    "def bpe_train(corpus_tokenized, vocab, V, bigrams_merged=None):    \n",
    "    \"\"\"Train a byte-pair encoding tokenizer.\n",
    "\n",
    "    Args:\n",
    "        corpus_tokenized  (list(list(str))): The list of tokens to be retokenized.\n",
    "        vocab (set): The vocabulary so far.\n",
    "        V (int): The desired vocabulary size.\n",
    "    \"\"\"\n",
    "\n",
    "    i = 1\n",
    "    if bigrams_merged is None:\n",
    "        bigrams_merged = list()\n",
    "    while len(vocab) < V:\n",
    "        print(f\"====Iteration {i}====\")\n",
    "        bigrams = count_bigrams_in_corpus(corpus_tokenized)\n",
    "        top_bigram = bigrams.most_common()[0][0]\n",
    "        bigrams_merged.append(top_bigram)\n",
    "        print(f\"Top bigram: {bigrams.most_common()[0]}\")\n",
    "        merge_bigram_in_corpus(corpus_tokenized, top_bigram)\n",
    "        print(f\"Corpus after merge:\")\n",
    "        print_tokenized_corpus(corpus_tokenized)\n",
    "        vocab.add(top_bigram[0] + top_bigram[1])\n",
    "        print(f\"Vocab after merge (size {len(vocab)}): {vocab}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return {\"corpus_tokenized\": corpus_tokenized, \"vocab\": vocab, \"bigrams_merged\": bigrams_merged}\n",
    "\n",
    "bpe_train(corpus_tokenized, vocab, V, bigrams_merged=bigrams_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized corpus:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # hik e s # .\n",
      "the # hik er # k i s s e s # the # stink in g # sink # .\n",
      "Final vocabulary:\n",
      "{'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 'hi', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k', 'hik'}\n",
      "Bigrams merged at each step:\n",
      "[('i', 'n'), ('t', 'h'), ('th', 'e'), ('in', 'k'), ('t', 'ink'), ('s', 'ink'), ('s', 'tink'), ('e', 'r'), ('h', 'i'), ('hi', 'k')]\n",
      "Tokenized corpus:\n",
      "the # sink # stink s # .\n",
      "s i t # in # the # e g g # .\n",
      "the # tink er # hik e s # .\n",
      "the # hik er # k i s s e s # the # stink in g # sink # .\n",
      "Final vocabulary:\n",
      "{'sink', 'ink', 'stink', 'h', '.', 'th', 'g', 'the', 'r', 'hi', 's', 'in', 't', 'er', 'e', 'i', 'n', 'tink', 'k', 'hik'}\n",
      "Bigrams merged at each step:\n",
      "[('i', 'n'), ('t', 'h'), ('th', 'e'), ('in', 'k'), ('t', 'ink'), ('s', 'ink'), ('s', 'tink'), ('e', 'r'), ('h', 'i'), ('hi', 'k')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized corpus:\")\n",
    "print_tokenized_corpus(corpus_tokenized)\n",
    "print(\"Final vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"Bigrams merged at each step:\")\n",
    "print(bigrams_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding: Tokenising a new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised new text:\n",
      "t h e # s i n k s # a r e # s t i n k y # .\n",
      "h e # k i s s e s # t h e # e g g # .\n",
      "Bigram merged: ('i', 'n')\n",
      "Corpus after merge:\n",
      "t h e # s in k s # a r e # s t in k y # .\n",
      "h e # k i s s e s # t h e # e g g # .\n",
      "Bigram merged: ('t', 'h')\n",
      "Corpus after merge:\n",
      "th e # s in k s # a r e # s t in k y # .\n",
      "h e # k i s s e s # th e # e g g # .\n",
      "Bigram merged: ('th', 'e')\n",
      "Corpus after merge:\n",
      "the # s in k s # a r e # s t in k y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('in', 'k')\n",
      "Corpus after merge:\n",
      "the # s ink s # a r e # s t ink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('t', 'ink')\n",
      "Corpus after merge:\n",
      "the # s ink s # a r e # s tink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('s', 'ink')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # s tink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('s', 'tink')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('e', 'r')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('h', 'i')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('hi', 'k')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Tokenised new text:\n",
      "t h e # s i n k s # a r e # s t i n k y # .\n",
      "h e # k i s s e s # t h e # e g g # .\n",
      "Bigram merged: ('i', 'n')\n",
      "Corpus after merge:\n",
      "t h e # s in k s # a r e # s t in k y # .\n",
      "h e # k i s s e s # t h e # e g g # .\n",
      "Bigram merged: ('t', 'h')\n",
      "Corpus after merge:\n",
      "th e # s in k s # a r e # s t in k y # .\n",
      "h e # k i s s e s # th e # e g g # .\n",
      "Bigram merged: ('th', 'e')\n",
      "Corpus after merge:\n",
      "the # s in k s # a r e # s t in k y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('in', 'k')\n",
      "Corpus after merge:\n",
      "the # s ink s # a r e # s t ink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('t', 'ink')\n",
      "Corpus after merge:\n",
      "the # s ink s # a r e # s tink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('s', 'ink')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # s tink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('s', 'tink')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('e', 'r')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('h', 'i')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n",
      "Bigram merged: ('hi', 'k')\n",
      "Corpus after merge:\n",
      "the # sink s # a r e # stink y # .\n",
      "h e # k i s s e s # the # e g g # .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['the', '#', 'sink', 's', '#', 'a', 'r', 'e', '#', 'stink', 'y', '#', '.'],\n",
       " ['h',\n",
       "  'e',\n",
       "  '#',\n",
       "  'k',\n",
       "  'i',\n",
       "  's',\n",
       "  's',\n",
       "  'e',\n",
       "  's',\n",
       "  '#',\n",
       "  'the',\n",
       "  '#',\n",
       "  'e',\n",
       "  'g',\n",
       "  'g',\n",
       "  '#',\n",
       "  '.']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bpe_tokenize_new_text(corpus_new, merge_steps):\n",
    "    \"\"\"Tokenize a new text using an existing BPE tokenizer.\n",
    "\n",
    "    Args:\n",
    "        corpus_new (list(str)):  A list of raw texts.\n",
    "        merge_steps (list(tuple(str))): A list of bigrams, representing\n",
    "                                        the bigrams to be merged, in the\n",
    "                                        order they are applied.\n",
    "\n",
    "    Returns:\n",
    "        list(list(str)): The tokenised texts.\n",
    "    \"\"\"\n",
    "    corpus_new_preprocessed = preprocess_corpus(corpus_new)\n",
    "    corpus_new_tokenized = get_init_tokenization(corpus_new_preprocessed)\n",
    "    print(f\"Tokenised new text:\")\n",
    "    print_tokenized_corpus(corpus_new_tokenized)\n",
    "    for bigram in merge_steps:\n",
    "        print(f\"Bigram merged: {bigram}\")\n",
    "        merge_bigram_in_corpus(corpus_new_tokenized, bigram)\n",
    "        print(f\"Corpus after merge:\")\n",
    "        print_tokenized_corpus(corpus_new_tokenized)\n",
    "    return corpus_new_tokenized\n",
    "\n",
    "\n",
    "corpus_new = [\"The sinks are stinky.\", \"He kisses the egg.\"]\n",
    "bpe_tokenize_new_text(corpus_new, bigrams_merged)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
